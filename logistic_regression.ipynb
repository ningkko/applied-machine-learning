{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "##  Spam Filtering with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0                                                  1\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "Number of spam messages: 747\n",
      "Number of ham messages: 4825\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/SMSSpamCollection/SMSSpamCollection', delimiter='\\t', header=None)\n",
    "print(df.head())\n",
    "print ('Number of spam messages:', df[df[0] == 'spam'][0].count())\n",
    "print ('Number of ham messages:', df[df[0] == 'ham'][0].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import sklearn.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/SMSSpamCollection/SMSSpamCollection', delimiter='\\t', header=None)\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(df[1], df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ham.\n",
      "Prediction: ham.\n",
      "Prediction: ham.\n",
      "Prediction: ham.\n",
      "Prediction: ham.\n",
      "5023    Yes its possible but dint try. Pls dont tell t...\n",
      "2913    You didn't have to tell me that...now i'm thin...\n",
      "5029    Go chase after her and run her over while she'...\n",
      "1489    Tell them no need to investigate about me anyw...\n",
      "110                        Dont worry. I guess he's busy.\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "for prediction in predictions[:5]:\n",
    "    print ('Prediction: %s.' % prediction)\n",
    "    \n",
    "print (X_test_raw[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metrics\n",
    "- Notes are somehere else, I'll just skip it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred, y_true = [0, 1, 1, 0], [1, 1, 1, 1]\n",
    "print ('Accuracy:', accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "- Precision and recall are two measurements usually used for evaluating classifiers due to their sensitivities of false/true negative/positive results.\n",
    "- Notes are somewhere else, skip\n",
    "\n",
    "### The F1 Measure\n",
    "- the harmonic mean, or weighted average, of the precision and recall scores. \n",
    "- It is also called the **f-measure** or the **f-score**.\n",
    "- It is calculated using the formula **F1= 2* P*R/(P+R)**\n",
    "- Penalizes classifiers with imbalanced precision and recall scores, like a classifier always returning positive predictions\n",
    "- If a model is perfectly balanced, the f1-score will be 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
