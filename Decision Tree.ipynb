{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tutorial: [Train an Image Classifier with TensorFlow for Poets - Machine Learning Recipes #6](https://www.youtube.com/watch?v=LDRbO9a6XPU&list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal&index=8)\n",
    "2. Read more about decision trees: [A useful view of decision trees](https://www.benkuhn.net/tree-imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree using CART\n",
    "- CART: Classification and Regression Trees\n",
    "    - Start with a node containing all the examples\n",
    "    - For all possible questions and threshold:\n",
    "        Ask a question with a threshold and Split the data\n",
    "        Calculate the information gain (which is calculated using the Gini-impurity factor).\n",
    "    - Choose the set of question and threshold with the highest information gain and least Gini impurity.\n",
    "    - Continue doing so till only one example is left in each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define two functions for rounding our data. We round the data because 10 digital numbers can produce too many differences, which makes it too hard to gain info from each question we ask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_zero_point_five(inp):\n",
    "    return (inp*2).round()/2\n",
    "\n",
    "def round_to_nearest_zero_point_two_five(inp):\n",
    "    return (inp*4).round()/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see that the more precise the data is, the more accurate the prediction is, but also the heavier the cost (running time) is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - The data we will use is Iris. First we concatenate X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris=datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(1,150)\n",
    "\n",
    "data = np.concatenate((X, y.T), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    \n",
    "    data_train, data_test = train_test_split(data, test_size=0.25, random_state=33)\n",
    "    print(\"Training data:\\n\", data_train)\n",
    "    print(\"Testing data:\\n\", data_test)\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      " [[5. 2. 3. 1. 1.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 2. 4. 1. 1.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [5. 2. 3. 1. 1.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [8. 4. 6. 2. 2.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [7. 3. 5. 1. 1.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [8. 3. 7. 2. 2.]\n",
      " [6. 2. 4. 1. 1.]\n",
      " [6. 3. 5. 2. 1.]\n",
      " [5. 4. 1. 0. 0.]\n",
      " [4. 2. 1. 0. 0.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [4. 3. 1. 0. 0.]\n",
      " [5. 4. 1. 0. 0.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [6. 3. 4. 2. 1.]\n",
      " [6. 2. 4. 2. 1.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [6. 2. 5. 2. 2.]\n",
      " [5. 4. 2. 1. 0.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [7. 3. 5. 1. 1.]\n",
      " [5. 3. 4. 1. 1.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [6. 2. 4. 1. 1.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [7. 3. 5. 1. 1.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [4. 3. 1. 0. 0.]\n",
      " [7. 4. 6. 2. 2.]\n",
      " [5. 4. 1. 0. 0.]\n",
      " [6. 2. 5. 2. 1.]\n",
      " [4. 3. 1. 0. 0.]\n",
      " [8. 3. 7. 2. 2.]\n",
      " [6. 3. 4. 2. 1.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [5. 4. 1. 0. 0.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [6. 4. 1. 0. 0.]\n",
      " [6. 4. 2. 0. 0.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [7. 3. 5. 2. 2.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [6. 4. 1. 0. 0.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [6. 2. 4. 1. 1.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [5. 2. 4. 1. 1.]\n",
      " [7. 3. 4. 1. 1.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [6. 2. 4. 1. 1.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 3. 4. 2. 1.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [7. 3. 5. 2. 1.]\n",
      " [7. 2. 6. 2. 2.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [5. 4. 1. 0. 0.]\n",
      " [8. 4. 7. 2. 2.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 2. 4. 1. 1.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 3. 5. 2. 1.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [6. 3. 5. 1. 1.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [5. 3. 4. 2. 1.]\n",
      " [5. 4. 1. 0. 0.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [5. 2. 4. 2. 2.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [6. 2. 4. 1. 1.]\n",
      " [8. 3. 7. 2. 2.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [7. 3. 5. 2. 1.]\n",
      " [6. 4. 1. 0. 0.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [6. 3. 4. 2. 1.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [5. 2. 3. 1. 1.]\n",
      " [6. 2. 5. 2. 2.]\n",
      " [6. 3. 4. 2. 1.]\n",
      " [8. 3. 6. 2. 2.]\n",
      " [5. 3. 2. 0. 0.]]\n",
      "Testing data:\n",
      " [[6. 3. 4. 1. 1.]\n",
      " [7. 3. 4. 1. 1.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 3. 5. 2. 1.]\n",
      " [6. 3. 6. 1. 2.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [5. 4. 1. 0. 0.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [4. 3. 1. 0. 0.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [6. 3. 5. 1. 1.]\n",
      " [7. 3. 5. 2. 2.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [5. 4. 2. 0. 0.]\n",
      " [6. 3. 5. 2. 1.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [5. 3. 2. 0. 0.]\n",
      " [6. 4. 2. 0. 0.]\n",
      " [6. 3. 6. 2. 2.]\n",
      " [5. 3. 1. 0. 0.]\n",
      " [6. 2. 5. 2. 2.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [6. 3. 5. 1. 1.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [6. 3. 4. 1. 1.]\n",
      " [7. 3. 5. 2. 1.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [7. 3. 6. 2. 2.]\n",
      " [7. 3. 5. 2. 2.]\n",
      " [6. 3. 5. 2. 2.]\n",
      " [6. 3. 4. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = split_data(data.round())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Header contains the content of questions we are going to ask for splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = iris.feature_names\n",
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So the question we will ask is something like \"Is the sepal length (cm) >= 3?\" and \"Is the petal width (cm) >= 2?\", with 3 and 2 here as the \"threshold values.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \"\"\"\n",
    "    The question class: all pairs of (j, t)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, question_number, threshold_value):\n",
    "        self.question_number = question_number\n",
    "        self.threshold_value = threshold_value\n",
    "\n",
    "    def match(self, example):\n",
    "        \"\"\"\n",
    "        returns true if the example's value to the question is greater or euqal to \n",
    "        the threshold value of the question\n",
    "        \"\"\"\n",
    "        value = example[self.question_number]\n",
    "        return value >= self.threshold_value\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Prints the process and result ina human-rigidable way\n",
    "        \"\"\"\n",
    "        condition = \">=\"\n",
    "        return \"Is %s %s %s?\" % (\n",
    "            header[self.question_number], condition, str(self.threshold_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Is sepal width (cm) >= 2?"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Question(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(dataset, question):\n",
    "    \"\"\"Partitions a dataset.\n",
    "\n",
    "    For each row in the dataset, check if it matches the question. If\n",
    "    so, add it to 'true rows', otherwise, add it to 'false rows'.\n",
    "    \"\"\"\n",
    "    true_example, false_example = [], []\n",
    "    for row in dataset:\n",
    "        if question.match(row):\n",
    "            true_example.append(row)\n",
    "        else:\n",
    "            false_example.append(row)\n",
    "    return true_example, false_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_labels(dataset):\n",
    "    \"\"\"\n",
    "    Counts for how many different labels the dataset has.\n",
    "    \"\"\"\n",
    "    counts = {}  \n",
    "    for data in dataset:\n",
    "        # Label is always the last one (since we define it this way)\n",
    "        label = data[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(dataset):\n",
    "    \"\"\"Calculates the Gini Impurity of a dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    counts = count_number_of_labels(dataset)\n",
    "    impurity = 1\n",
    "    for label in counts:\n",
    "        prob_of_label = counts[label] / float(len(dataset))\n",
    "        impurity -= prob_of_label**2\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6399999999999999"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test\n",
    "gini([[1],[2],[2],[1],[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(left_child, right_child, current_uncertainty):\n",
    "    \"\"\"\n",
    "    Information Gain = parent_node.uncertainty - (left_child.gini_impurity + right_child.gini_impurity)\n",
    "    \"\"\"\n",
    "    left_child_percentage = float(len(left_child)) / (len(left_child) + len(right_child))\n",
    "    right_child_percentage = 1 - left_child_percentage\n",
    "    return current_uncertainty - left_child_percentage * gini(left_child) - right_child_percentage * gini(right_child)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(dataset):\n",
    "    \"\"\"Find the best question to ask by iterating over every feature / value\n",
    "    and calculating the information gain.\"\"\"\n",
    "    best_gain = 0 \n",
    "    best_question = None\n",
    "    current_uncertainty = gini(dataset)\n",
    "    \n",
    "    feature_number = len(dataset[0]) - 1  \n",
    "\n",
    "    ## now for each pair of (j, t), which means (question and threshold), try\n",
    "    ## splitting the set and calculating the information gain.\n",
    "    \n",
    "    for question_number in range(feature_number):\n",
    "        \n",
    "        thresholds = set([row[question_number] for row in dataset])\n",
    "        \n",
    "        for threshold in thresholds:  \n",
    "            \n",
    "            question = Question(question_number, threshold)\n",
    "            true_examples, false_examples = partition(dataset, question)\n",
    "            \n",
    "            ## skip if a pair of (j, t) fails in splitting the set\n",
    "            if len(true_examples) == 0 or len(false_examples) == 0:\n",
    "                continue\n",
    "\n",
    "            info_gain = information_gain(true_examples, false_examples, current_uncertainty)\n",
    "\n",
    "            if info_gain >= best_gain:\n",
    "                best_gain, best_question = info_gain, question\n",
    "\n",
    "    return best_gain, best_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gain:  0.35309311224489803 \n",
      "Best question:  Is petal length (cm) >= 3.0?\n"
     ]
    }
   ],
   "source": [
    "best_gain, best_question = find_best_split(data_train)\n",
    "print(\"Best gain: \", best_gain, \n",
    "      \"\\nBest question: \", best_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    \"\"\"\n",
    "    a leaf is a list of examples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.predictions = count_number_of_labels(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Node:\n",
    "    \"\"\"A Decision Node asks a question.\n",
    "\n",
    "    This holds a reference to the question, and to the two child nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, question, true_branch, false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(dataset):\n",
    "\n",
    "    info_gain, question = find_best_split(dataset)\n",
    "\n",
    "    ## base case\n",
    "    if info_gain == 0:\n",
    "        return Leaf(dataset)\n",
    "    \n",
    "    true_examples, false_examples = partition(dataset, question)\n",
    "\n",
    "    true_branch = build_tree(true_examples)\n",
    "    false_branch = build_tree(false_examples)\n",
    "\n",
    "    return Decision_Node(question, true_branch, false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, spacing=\"\"):\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        print (spacing + \"Predict\", node.predictions)\n",
    "        return\n",
    "\n",
    "    # Print the question at this node\n",
    "    print (spacing + \"|\" + str(node.question))\n",
    "\n",
    "    # Call this function recursively on the true branch\n",
    "    print (spacing + '--> True:')\n",
    "    print_tree(node.true_branch, spacing + \"  \")\n",
    "\n",
    "    # Call this function recursively on the false branch\n",
    "    print (spacing + '--> False:')\n",
    "    print_tree(node.false_branch, spacing + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data_test,classifier):\n",
    "    def classify_one(row, node):\n",
    "        \"\"\"See the 'rules of recursion' above.\"\"\"\n",
    "\n",
    "        # Base case: we've reached a leaf\n",
    "        if isinstance(node, Leaf):\n",
    "            return node.predictions\n",
    "\n",
    "        if node.question.match(row):\n",
    "            return classify_one(row, node.true_branch)\n",
    "        else:\n",
    "            return classify_one(row, node.false_branch)\n",
    "    \n",
    "    def print_leaf(counts):\n",
    "        total = sum(counts.values())\n",
    "        probs = {}\n",
    "        for labels in counts.keys():\n",
    "            probs[labels] = str(int(counts[labels] / total * 100)) + \"%\"\n",
    "        return probs\n",
    "\n",
    "    \n",
    "    for row in data_test:\n",
    "        print (\"Actual: %s. Predicted: %s\" %\n",
    "               (row[-1], print_leaf(classify_one(row,classifier))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Is petal length (cm) >= 3.0?\n",
      "--> True:\n",
      "  |Is petal length (cm) >= 5.0?\n",
      "  --> True:\n",
      "    |Is petal width (cm) >= 2.0?\n",
      "    --> True:\n",
      "      |Is petal length (cm) >= 6.0?\n",
      "      --> True:\n",
      "        Predict {2.0: 19}\n",
      "      --> False:\n",
      "        |Is sepal length (cm) >= 7.0?\n",
      "        --> True:\n",
      "          Predict {2.0: 1, 1.0: 2}\n",
      "        --> False:\n",
      "          |Is sepal width (cm) >= 3.0?\n",
      "          --> True:\n",
      "            Predict {2.0: 8, 1.0: 2}\n",
      "          --> False:\n",
      "            Predict {2.0: 2, 1.0: 1}\n",
      "    --> False:\n",
      "      Predict {1.0: 4}\n",
      "  --> False:\n",
      "    |Is sepal length (cm) >= 6.0?\n",
      "    --> True:\n",
      "      Predict {1.0: 24}\n",
      "    --> False:\n",
      "      |Is petal width (cm) >= 2.0?\n",
      "      --> True:\n",
      "        |Is sepal width (cm) >= 3.0?\n",
      "        --> True:\n",
      "          Predict {1.0: 1}\n",
      "        --> False:\n",
      "          Predict {2.0: 1}\n",
      "      --> False:\n",
      "        Predict {1.0: 5}\n",
      "--> False:\n",
      "  Predict {0.0: 42}\n"
     ]
    }
   ],
   "source": [
    "decision_tree_classifier = build_tree(data_train)\n",
    "print_tree(decision_tree_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 1.0. Predicted: {2.0: '80%', 1.0: '20%'}\n",
      "Actual: 2.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '33%', 1.0: '66%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '80%', 1.0: '20%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 1.0. Predicted: {2.0: '80%', 1.0: '20%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '66%', 1.0: '33%'}\n",
      "Actual: 2.0. Predicted: {2.0: '80%', 1.0: '20%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '80%', 1.0: '20%'}\n",
      "Actual: 2.0. Predicted: {2.0: '80%', 1.0: '20%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 1.0. Predicted: {2.0: '33%', 1.0: '66%'}\n",
      "Actual: 2.0. Predicted: {2.0: '80%', 1.0: '20%'}\n",
      "Actual: 2.0. Predicted: {2.0: '80%', 1.0: '20%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '33%', 1.0: '66%'}\n",
      "Actual: 2.0. Predicted: {2.0: '80%', 1.0: '20%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n"
     ]
    }
   ],
   "source": [
    "classify(data_test, decision_tree_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we round the data to its nearest integer, we have 13 wrong classifications.\n",
    "\n",
    "Now let's round the data into nearest 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      " [[5.  2.5 3.5 1.  1. ]\n",
      " [5.  3.  1.5 0.  0. ]\n",
      " [6.5 2.5 4.5 1.5 1. ]\n",
      " [6.  2.5 4.  1.  1. ]\n",
      " [6.  3.  4.5 1.5 1. ]\n",
      " [4.5 3.  1.5 0.  0. ]\n",
      " [4.5 3.5 1.5 0.5 0. ]\n",
      " [5.  2.5 3.  1.  1. ]\n",
      " [5.  3.5 1.5 0.  0. ]\n",
      " [8.  4.  6.5 2.  2. ]\n",
      " [5.  3.5 1.5 0.  0. ]\n",
      " [5.  3.5 1.5 0.5 0. ]\n",
      " [5.5 3.  3.5 1.5 1. ]\n",
      " [6.5 3.  5.5 2.  2. ]\n",
      " [5.5 4.  1.5 0.5 0. ]\n",
      " [7.  3.  4.5 1.5 1. ]\n",
      " [6.  3.  5.  2.5 2. ]\n",
      " [7.5 2.5 7.  2.5 2. ]\n",
      " [5.5 2.5 4.  1.5 1. ]\n",
      " [6.  3.  5.  2.  1. ]\n",
      " [5.  3.5 1.5 0.  0. ]\n",
      " [4.5 2.5 1.5 0.5 0. ]\n",
      " [6.5 3.  5.  1.5 2. ]\n",
      " [4.5 3.  1.5 0.  0. ]\n",
      " [5.  3.5 1.5 0.  0. ]\n",
      " [7.  3.  6.  1.5 2. ]\n",
      " [6.  3.5 4.5 1.5 1. ]\n",
      " [6.  2.  4.5 1.5 1. ]\n",
      " [7.5 3.  6.  2.  2. ]\n",
      " [7.  3.  5.5 2.  2. ]\n",
      " [6.5 3.  5.5 2.  2. ]\n",
      " [5.5 2.5 5.  2.  2. ]\n",
      " [5.  3.5 1.5 0.5 0. ]\n",
      " [5.  4.  1.5 0.5 0. ]\n",
      " [7.  3.  5.  1.5 1. ]\n",
      " [5.  2.5 4.  1.5 1. ]\n",
      " [5.  4.  1.5 0.  0. ]\n",
      " [5.5 2.5 4.  1.  1. ]\n",
      " [6.  3.  5.  2.  2. ]\n",
      " [6.5 3.  4.5 1.5 1. ]\n",
      " [6.5 3.  5.5 2.  2. ]\n",
      " [4.5 3.  1.  0.  0. ]\n",
      " [7.  3.5 6.  2.5 2. ]\n",
      " [5.  3.5 1.5 0.  0. ]\n",
      " [6.5 2.5 5.  1.5 1. ]\n",
      " [4.5 3.  1.5 0.  0. ]\n",
      " [7.5 3.  6.5 2.  2. ]\n",
      " [6.5 3.  4.5 1.5 1. ]\n",
      " [5.  4.  2.  0.5 0. ]\n",
      " [6.5 3.  5.  2.  2. ]\n",
      " [6.5 2.5 5.  2.  2. ]\n",
      " [5.  3.5 1.5 0.5 0. ]\n",
      " [5.  3.5 1.5 0.  0. ]\n",
      " [6.5 2.5 5.5 2.  2. ]\n",
      " [6.  4.  1.  0.  0. ]\n",
      " [5.5 4.5 1.5 0.5 0. ]\n",
      " [4.5 3.  1.5 0.  0. ]\n",
      " [7.  3.  5.5 2.  2. ]\n",
      " [6.5 3.  5.5 2.  2. ]\n",
      " [5.5 3.5 1.5 0.  0. ]\n",
      " [5.5 2.5 3.5 1.  1. ]\n",
      " [5.  3.5 1.5 0.  0. ]\n",
      " [6.  2.  4.  1.  1. ]\n",
      " [5.  3.  1.5 0.  0. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [6.5 3.  4.5 1.5 1. ]\n",
      " [5.5 3.  4.5 1.5 1. ]\n",
      " [5.5 2.5 4.5 1.  1. ]\n",
      " [5.5 2.5 4.  1.5 1. ]\n",
      " [5.  3.  1.5 0.  0. ]\n",
      " [6.  3.  4.5 1.5 1. ]\n",
      " [5.  3.  1.5 0.  0. ]\n",
      " [7.  3.  5.  1.5 1. ]\n",
      " [6.5 2.5 6.  2.  2. ]\n",
      " [5.5 3.5 1.5 0.  0. ]\n",
      " [5.  4.  1.5 0.  0. ]\n",
      " [5.  3.5 1.5 0.5 0. ]\n",
      " [4.5 3.5 1.  0.  0. ]\n",
      " [7.5 4.  6.5 2.  2. ]\n",
      " [7.5 3.  6.5 2.  2. ]\n",
      " [4.5 3.  1.5 0.  0. ]\n",
      " [5.5 2.5 3.5 1.  1. ]\n",
      " [6.  2.5 4.  1.  1. ]\n",
      " [6.5 3.5 5.5 2.  2. ]\n",
      " [6.  2.5 4.  1.  1. ]\n",
      " [5.5 3.5 1.5 0.  0. ]\n",
      " [5.  3.5 1.5 0.  0. ]\n",
      " [6.  2.5 5.  1.5 1. ]\n",
      " [5.5 3.  4.  1.5 1. ]\n",
      " [6.  3.  4.5 1.  1. ]\n",
      " [5.  3.5 1.5 0.5 0. ]\n",
      " [5.5 3.  4.  1.5 1. ]\n",
      " [5.5 3.  4.5 1.5 1. ]\n",
      " [5.5 4.  1.5 0.5 0. ]\n",
      " [6.5 3.5 5.5 2.5 2. ]\n",
      " [5.  2.5 4.5 1.5 2. ]\n",
      " [6.  3.5 5.5 2.5 2. ]\n",
      " [5.5 2.5 4.  1.  1. ]\n",
      " [7.5 3.  6.5 2.  2. ]\n",
      " [5.5 3.5 1.5 0.5 0. ]\n",
      " [6.5 3.  5.  1.5 1. ]\n",
      " [5.5 4.  1.5 0.  0. ]\n",
      " [5.  3.  1.  0.  0. ]\n",
      " [5.  3.  1.5 0.5 0. ]\n",
      " [6.  2.5 5.  2.  2. ]\n",
      " [6.  3.  4.  1.5 1. ]\n",
      " [7.  3.  6.  2.  2. ]\n",
      " [5.  2.5 3.5 1.  1. ]\n",
      " [6.5 2.5 5.  2.  2. ]\n",
      " [5.5 3.  4.5 1.5 1. ]\n",
      " [7.5 3.  6.  2.5 2. ]\n",
      " [5.5 3.5 1.5 0.  0. ]]\n",
      "Testing data:\n",
      " [[5.5 3.  4.  1.5 1. ]\n",
      " [6.5 3.  4.5 1.5 1. ]\n",
      " [4.5 3.  1.5 0.  0. ]\n",
      " [6.5 3.  4.5 1.5 1. ]\n",
      " [6.  2.5 5.5 1.5 2. ]\n",
      " [6.5 3.5 6.  2.5 2. ]\n",
      " [5.  3.5 2.  0.  0. ]\n",
      " [5.  3.5 1.5 0.5 0. ]\n",
      " [6.5 3.  5.5 2.  2. ]\n",
      " [7.  3.  5.5 2.5 2. ]\n",
      " [7.  3.  6.  2.5 2. ]\n",
      " [4.5 3.  1.5 0.  0. ]\n",
      " [6.5 3.5 5.5 2.5 2. ]\n",
      " [6.  3.  4.5 1.5 1. ]\n",
      " [7.  3.  5.  2.5 2. ]\n",
      " [6.5 3.  4.5 1.5 1. ]\n",
      " [6.  3.  5.  2.  2. ]\n",
      " [5.  3.5 1.5 0.  0. ]\n",
      " [6.5 3.5 4.5 1.5 1. ]\n",
      " [7.  3.  6.  2.  2. ]\n",
      " [5.  3.  1.5 0.  0. ]\n",
      " [5.5 4.  1.5 0.5 0. ]\n",
      " [6.5 3.  6.  2.  2. ]\n",
      " [5.  3.  1.5 0.  0. ]\n",
      " [6.  2.  5.  1.5 2. ]\n",
      " [6.  3.  5.  2.  2. ]\n",
      " [6.  3.  4.5 1.5 1. ]\n",
      " [6.  3.  4.  1.5 1. ]\n",
      " [6.5 3.  5.  2.  2. ]\n",
      " [6.  3.  5.  2.  2. ]\n",
      " [5.5 2.5 4.  1.5 1. ]\n",
      " [6.5 3.  4.5 1.5 1. ]\n",
      " [5.5 3.  5.  2.  2. ]\n",
      " [6.5 3.  5.5 2.5 2. ]\n",
      " [6.5 3.  5.5 2.5 2. ]\n",
      " [6.5 3.  5.  2.5 2. ]\n",
      " [6.  2.5 5.  2.  2. ]\n",
      " [5.5 3.  4.  1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = split_data(round_to_nearest_zero_point_five(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {1.0: '50%', 2.0: '50%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 2.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {1.0: '50%', 2.0: '50%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {1.0: '50%', 2.0: '50%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {1.0: '50%', 2.0: '50%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n"
     ]
    }
   ],
   "source": [
    "decision_tree_classifier = build_tree(data_train)\n",
    "classify(data_test, decision_tree_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 4 classifications were wrong.\n",
    "\n",
    "Let's try rounding to nearest 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      " [[5.   2.25 3.25 1.   1.  ]\n",
      " [5.   3.   1.5  0.25 0.  ]\n",
      " [6.25 2.25 4.5  1.25 1.  ]\n",
      " [5.75 2.5  4.   1.25 1.  ]\n",
      " [6.25 3.   4.25 1.25 1.  ]\n",
      " [4.75 3.25 1.25 0.25 0.  ]\n",
      " [4.5  3.5  1.5  0.25 0.  ]\n",
      " [5.   2.5  3.   1.   1.  ]\n",
      " [4.75 3.5  1.5  0.25 0.  ]\n",
      " [8.   3.75 6.5  2.   2.  ]\n",
      " [5.   3.5  1.5  0.25 0.  ]\n",
      " [5.   3.75 1.5  0.5  0.  ]\n",
      " [5.5  3.   3.5  1.25 1.  ]\n",
      " [6.5  3.   5.5  1.75 2.  ]\n",
      " [5.5  4.   1.75 0.5  0.  ]\n",
      " [7.   3.25 4.75 1.5  1.  ]\n",
      " [5.75 2.75 5.   2.5  2.  ]\n",
      " [7.75 2.5  7.   2.25 2.  ]\n",
      " [5.5  2.5  4.   1.25 1.  ]\n",
      " [6.   3.25 4.75 1.75 1.  ]\n",
      " [5.   3.5  1.5  0.   0.  ]\n",
      " [4.5  2.25 1.25 0.25 0.  ]\n",
      " [6.25 2.75 5.   1.5  2.  ]\n",
      " [4.5  3.   1.5  0.25 0.  ]\n",
      " [5.   3.5  1.5  0.25 0.  ]\n",
      " [7.25 3.   5.75 1.5  2.  ]\n",
      " [6.   3.5  4.5  1.5  1.  ]\n",
      " [6.25 2.25 4.5  1.5  1.  ]\n",
      " [7.5  2.75 6.   2.   2.  ]\n",
      " [6.75 3.   5.5  2.   2.  ]\n",
      " [6.5  2.75 5.5  2.25 2.  ]\n",
      " [5.75 2.5  5.   2.   2.  ]\n",
      " [5.   3.5  1.5  0.5  0.  ]\n",
      " [5.   3.75 1.5  0.25 0.  ]\n",
      " [6.75 2.75 4.75 1.5  1.  ]\n",
      " [5.25 2.75 4.   1.5  1.  ]\n",
      " [5.   3.75 1.5  0.25 0.  ]\n",
      " [5.5  2.5  4.   1.   1.  ]\n",
      " [6.   3.   5.   1.75 2.  ]\n",
      " [6.5  3.   4.5  1.25 1.  ]\n",
      " [6.5  2.75 5.5  2.   2.  ]\n",
      " [4.25 3.   1.   0.   0.  ]\n",
      " [7.25 3.5  6.   2.5  2.  ]\n",
      " [5.   3.5  1.5  0.25 0.  ]\n",
      " [6.25 2.5  5.   1.5  1.  ]\n",
      " [4.5  3.25 1.25 0.25 0.  ]\n",
      " [7.5  3.   6.5  2.   2.  ]\n",
      " [6.5  3.25 4.5  1.5  1.  ]\n",
      " [5.   3.75 2.   0.5  0.  ]\n",
      " [6.5  3.25 5.   2.   2.  ]\n",
      " [6.25 2.75 5.   1.75 2.  ]\n",
      " [5.   3.5  1.25 0.25 0.  ]\n",
      " [5.25 3.5  1.5  0.25 0.  ]\n",
      " [6.5  2.75 5.25 2.   2.  ]\n",
      " [5.75 4.   1.25 0.25 0.  ]\n",
      " [5.75 4.5  1.5  0.5  0.  ]\n",
      " [4.5  3.25 1.5  0.25 0.  ]\n",
      " [7.   3.   5.5  2.   2.  ]\n",
      " [6.25 3.   5.5  1.75 2.  ]\n",
      " [5.5  3.5  1.25 0.25 0.  ]\n",
      " [5.75 2.5  3.5  1.   1.  ]\n",
      " [5.   3.25 1.5  0.25 0.  ]\n",
      " [6.   2.25 4.   1.   1.  ]\n",
      " [5.   3.   1.5  0.25 0.  ]\n",
      " [5.   2.   3.5  1.   1.  ]\n",
      " [6.5  3.   4.5  1.5  1.  ]\n",
      " [5.75 2.75 4.5  1.25 1.  ]\n",
      " [5.5  2.5  4.5  1.25 1.  ]\n",
      " [5.5  2.25 4.   1.25 1.  ]\n",
      " [4.75 3.   1.5  0.25 0.  ]\n",
      " [6.   3.   4.5  1.5  1.  ]\n",
      " [5.   3.   1.5  0.25 0.  ]\n",
      " [7.   3.   5.   1.5  1.  ]\n",
      " [6.75 2.5  5.75 1.75 2.  ]\n",
      " [5.25 3.75 1.5  0.25 0.  ]\n",
      " [5.25 4.   1.5  0.   0.  ]\n",
      " [5.   3.5  1.5  0.5  0.  ]\n",
      " [4.5  3.5  1.   0.25 0.  ]\n",
      " [7.75 3.75 6.75 2.25 2.  ]\n",
      " [7.25 3.   6.25 1.75 2.  ]\n",
      " [4.5  3.   1.5  0.25 0.  ]\n",
      " [5.5  2.5  3.75 1.   1.  ]\n",
      " [5.75 2.75 4.   1.25 1.  ]\n",
      " [6.75 3.25 5.75 2.   2.  ]\n",
      " [5.75 2.75 4.   1.   1.  ]\n",
      " [5.5  3.75 1.5  0.25 0.  ]\n",
      " [5.   3.5  1.5  0.25 0.  ]\n",
      " [6.   2.75 5.   1.5  1.  ]\n",
      " [5.75 2.75 4.   1.25 1.  ]\n",
      " [6.   2.75 4.75 1.25 1.  ]\n",
      " [5.   3.25 1.75 0.5  0.  ]\n",
      " [5.5  3.   4.   1.25 1.  ]\n",
      " [5.5  3.   4.5  1.5  1.  ]\n",
      " [5.5  4.   1.25 0.5  0.  ]\n",
      " [6.75 3.25 5.75 2.5  2.  ]\n",
      " [5.   2.5  4.5  1.75 2.  ]\n",
      " [6.25 3.5  5.5  2.25 2.  ]\n",
      " [5.5  2.5  3.75 1.   1.  ]\n",
      " [7.75 2.75 6.75 2.   2.  ]\n",
      " [5.5  3.5  1.5  0.5  0.  ]\n",
      " [6.75 3.   5.   1.75 1.  ]\n",
      " [5.5  4.25 1.5  0.25 0.  ]\n",
      " [5.   3.25 1.25 0.25 0.  ]\n",
      " [4.75 3.   1.5  0.25 0.  ]\n",
      " [5.75 2.75 5.   2.   2.  ]\n",
      " [6.   3.   4.25 1.5  1.  ]\n",
      " [7.   3.   6.   2.   2.  ]\n",
      " [5.   2.5  3.25 1.   1.  ]\n",
      " [6.25 2.5  5.   2.   2.  ]\n",
      " [5.5  3.   4.5  1.5  1.  ]\n",
      " [7.75 3.   6.   2.25 2.  ]\n",
      " [5.5  3.5  1.75 0.25 0.  ]]\n",
      "Testing data:\n",
      " [[5.75 3.   4.25 1.25 1.  ]\n",
      " [6.75 3.   4.5  1.5  1.  ]\n",
      " [4.75 3.25 1.5  0.25 0.  ]\n",
      " [6.5  2.75 4.5  1.5  1.  ]\n",
      " [6.   2.5  5.5  1.5  2.  ]\n",
      " [6.25 3.25 6.   2.5  2.  ]\n",
      " [4.75 3.5  2.   0.25 0.  ]\n",
      " [5.   3.5  1.5  0.25 0.  ]\n",
      " [6.5  3.   5.5  1.75 2.  ]\n",
      " [7.   3.25 5.75 2.25 2.  ]\n",
      " [6.75 3.25 6.   2.25 2.  ]\n",
      " [4.5  3.   1.25 0.25 0.  ]\n",
      " [6.25 3.5  5.5  2.5  2.  ]\n",
      " [6.   3.   4.75 1.5  1.  ]\n",
      " [7.   3.   5.   2.25 2.  ]\n",
      " [6.5  3.   4.25 1.25 1.  ]\n",
      " [6.   3.   4.75 1.75 2.  ]\n",
      " [5.25 3.5  1.5  0.25 0.  ]\n",
      " [6.25 3.25 4.75 1.5  1.  ]\n",
      " [7.25 3.25 6.   1.75 2.  ]\n",
      " [5.   3.   1.5  0.   0.  ]\n",
      " [5.75 3.75 1.75 0.25 0.  ]\n",
      " [6.5  3.   5.75 2.25 2.  ]\n",
      " [4.75 3.   1.5  0.   0.  ]\n",
      " [6.   2.25 5.   1.5  2.  ]\n",
      " [6.25 2.75 4.75 1.75 2.  ]\n",
      " [6.   3.   4.5  1.5  1.  ]\n",
      " [6.   2.75 4.   1.25 1.  ]\n",
      " [6.5  3.   5.25 2.   2.  ]\n",
      " [6.   3.   5.   1.75 2.  ]\n",
      " [5.5  2.75 4.25 1.25 1.  ]\n",
      " [6.75 3.   4.75 1.5  1.  ]\n",
      " [5.5  2.75 5.   2.   2.  ]\n",
      " [6.5  3.25 5.25 2.25 2.  ]\n",
      " [6.75 3.   5.5  2.5  2.  ]\n",
      " [6.75 3.   5.25 2.25 2.  ]\n",
      " [5.75 2.75 5.   2.   2.  ]\n",
      " [5.75 3.   4.25 1.25 1.  ]]\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = split_data(round_to_nearest_zero_point_two_five(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {1.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 0.0. Predicted: {0.0: '100%'}\n",
      "Actual: 2.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {1.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 2.0. Predicted: {2.0: '100%'}\n",
      "Actual: 1.0. Predicted: {1.0: '100%'}\n"
     ]
    }
   ],
   "source": [
    "decision_tree_classifier = build_tree(data_train)\n",
    "classify(data_test, decision_tree_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the classifications got right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8947368421052632"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
